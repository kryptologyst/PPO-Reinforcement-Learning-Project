# PPO Configuration
ppo:
  # Training parameters
  episodes: 1000
  batch_steps: 2048
  ppo_epochs: 10
  clip_epsilon: 0.2
  gamma: 0.99
  gae_lambda: 0.95
  
  # Learning rates
  actor_lr: 3e-4
  critic_lr: 3e-4
  
  # Regularization
  entropy_coef: 0.01
  value_coef: 0.5
  max_grad_norm: 0.5
  
  # Network architecture
  hidden_sizes: [128, 128]
  activation: "relu"

# Environment settings
environment:
  name: "CartPole-v1"
  render_mode: null  # Set to "human" for visualization
  max_episode_steps: 500

# Logging and monitoring
logging:
  log_dir: "logs"
  tensorboard: true
  wandb:
    enabled: false
    project: "ppo-rl-project"
    entity: null
  
# Checkpointing
checkpoints:
  save_frequency: 100  # Save every N episodes
  save_dir: "checkpoints"
  keep_last: 5

# Evaluation
evaluation:
  eval_frequency: 50  # Evaluate every N episodes
  eval_episodes: 10
  render_eval: false

# Device settings
device: "auto"  # "auto", "cpu", "cuda", "mps"
